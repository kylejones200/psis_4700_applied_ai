# Week 2
# Regression and Classification

---

## Prediction Basics

We use regression for numbers and classification for categories.  
Both rely on past data to predict future values

---

## Linear Regression

Fit a line that minimizes squared error.  
Equation: y = β₀ + β₁x + ε

---

## Logistic Regression

Predicts probability of belonging to a class.  
Uses a sigmoid curve instead of a straight line

---

## Decision Trees

Split data by features to make decisions.  
Interpretation is easy but overfitting is common

---

## Random Forests

Combine many trees to reduce variance.  
The ensemble outperforms a single tree

---

## Model Evaluation

Use MAE, RMSE, Accuracy, Precision, Recall.  
Never trust training accuracy alone

---

## Overfitting and Regularization

Simple models generalize better.  
Use Ridge or Lasso to constrain coefficients

---

## Python Example Outline

Load dataset, split train/test, fit regression, visualize residuals.  
Demonstrate predictions with unseen data

---

## Summary

Prediction connects data to decision.  
Interpretability matters as much as performance

