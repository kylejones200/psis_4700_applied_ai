# Week 5
# Generative AI

---

## From Recognition to Generation

Generative models create text, images, or sound from patterns

---

## Transformer Architecture

Self-attention lets each token view the entire sequence.  
Scales efficiently across GPUs

---

## Encoder vs Decoder

Encoders summarize input  
Decoders produce output
GPT uses the decoder half

---

## Text-to-Text Tasks

Translation, summarization, reasoning, and creative writing

---

## Text-to-Image Tasks

Diffusion models like Stable Diffusion and DALL-E synthesize images

---

## Prompt Design

Specific instructions yield consistent results.  
Temperature controls randomness

---

## Fine-Tuning vs Grounding

Fine-tuning adjusts model weights.  
Grounding injects new context without retraining

---

## Example: Idea Generator App

Take user input → call model API → output new product ideas

---

## Summary

Generative AI learns probability of the next token.  
Prompt quality determines output quality

