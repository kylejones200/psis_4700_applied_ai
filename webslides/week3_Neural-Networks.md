# Week 3
# Neural Networks

---

## From Perceptron to Deep Learning

A perceptron mimics a neuron.  
Many layers create a neural network

---

## Structure of a Network

Input layer receives data.  
Hidden layers transform it.  
Output layer predicts the result

---

## Activation Functions

ReLU, sigmoid, and tanh decide when neurons "fire."  
They add non-linearity to the model

---

## Gradient Descent

Networks learn by adjusting weights to minimize loss.  
Backpropagation computes these gradients efficiently

---

## Overfitting and Dropout

Dropout disables random neurons during training.  
It improves generalization

---

## Example: Image Classification

Show MNIST digits.  
Demonstrate convolution and pooling

---

## Visualization

Display network architecture with flow from left (input) to right (output).  
Highlight layers in color

---

## Summary

Neural networks approximate complex functions.  
They are flexible but require data, compute, and care

